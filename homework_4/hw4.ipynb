{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Sentence Segmentation\n",
    "Spencer Hann  \n",
    "CS 562 | Winter 2019\n",
    "\n",
    "Sentence segmentation is a crucial, yet under-appreciated, part of any text processing pipeline. In any real-world setting, text will not come to you pre-chunked into sentences, and if your pipeline involves any sentence-level analysis (parsing, translation, entity & co-reference extraction, etc.) the first thing you'll need to do will be split things up into sentences. This is a deceptively challenging problem, as punctuation can be used in multiple ways. Consider the following sentence (borrowed from Kyle Gorman's [excellent discussion of the subject](http://www.wellformedness.com/blog/simpler-sentence-boundary-detection/)): \n",
    "\n",
    "> _\"Rolls-Royce Motor Cars Inc. said it expects its U.S. sales to remain steady at about 1,200 cars in 1990.\"_ \n",
    "\n",
    "This sentence features several periods, but only one of them (the last one) represents a valid sentence break. Sentences can also have embedded clauses or quotation marks, some of which may superficially look like a sentence boundary: \n",
    "\n",
    "> _He says the big questions–“Do you really need this much money to put up these investments? Have you told investors what is happening in your sector? What about your track record?–“aren’t asked of companies coming to market._\n",
    "\n",
    "There are many ways to approach this particular problem; it can be approached using rules, which themselves may be encoded as regular expressions, or (more usefully) it can be modeled as a machine learning problem. Specifically, as a binary classification problem, where candidate sentence boundaries (i.e., punctuation marks from the closed class of English punctuation that can represent a sentence boundary) are either _positive_ examples (actual sentence boundaries) or _false_ (not sentence boundaries). \n",
    "\n",
    "In this assignment, you will develop and test your own sentence boundary detection algorithm. This assignment is shorter than our previous assignments, and is also much more open-ended. By this point in the class, you have been exposed to several different families of technique for text classification and analysis, and have hopefully begun to build up some intuitions. For this assignment, you can choose whatever methods you like: log-linear models, rule-based methods, some sort of neural network, it's up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4_utils import data\n",
    "from hw4_utils import classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a version of the WSJ section of the Penn Treebank to use for training and evaluation purposes. See `data/wsj_sbd/README.md` for a description of where this data came from and how it was assembled.\n",
    "\n",
    "The `hw4_utils.data` module has a useful method for reading in data (`file_as_string`) and another utility function to extract candidate boundaries. Let's use these to explore our data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = data.file_as_string(\"data/wsj_sbd/dev.txt\")\n",
    "train = data.file_as_string(\"data/wsj_sbd/train.txt\")\n",
    "test = data.file_as_string(\"data/wsj_sbd/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\\nMr. Vinken is c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these is a string containing the text, with one input sentence per line. Note that newline characters are still present, which will be important as this will provide us with our ground truth.\n",
    "\n",
    "Remember, we _only_ will use the `test` split for our final evaluation. For data exploration, feature engineering, and for tuning hyperparameters, use the `dev` split.\n",
    "\n",
    "The first and most important thing to find out when doing classification is what your class probabilities are, so let's look into that. The `load_candidates()` function in the `hw4_utils.data` module will read an input data split, identify candidate boundaries, and extract information about the candidates context that we may want to use for feature engineering. One of the attributes it will extract is whether or not the candidate is actually a true sentence break (`is_true_break`). Obviously, this would only be useful at training time, as in the \"real world\" we wouldn't know the answer. :-)\n",
    "\n",
    "We will use `load_candidates()` to answer our question about class balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 1766, True: 5769})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([c.is_true_break for c in data.load_candidates(dev)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is often the case, our classes are quite imbalanced! Let's load in our ground truth labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = [o.is_true_break for o in data.load_candidates(dev)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can we find out about our candidate breaks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = list(data.load_candidates(dev[:10000])) # for now, just get candidates from the first 10k characters\n",
    "first_can = candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original context:  d as a nonexecutive director Nov. 29. Mr. Vinken is chairman of El\n",
      "punctuation mark:  .\n",
      "token to the left:  Nov\n",
      "token to the right:  29\n"
     ]
    }
   ],
   "source": [
    "print(\"original context: \",first_can.orig_obs)\n",
    "print(\"punctuation mark: \", first_can.punctuation_mark)\n",
    "print(\"token to the left: \", first_can.left_token)\n",
    "print(\"token to the right: \", first_can.right_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the the `Observation` namedtuple in `hw4_utils.data` for more details about the various attributes that you can find out about each possible sentence boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baselines\n",
    "\n",
    "We will begin by looking at three different baselines. First, we will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline #1: Majority class\n",
    "\n",
    "Since our data are quite imbalanced, what if we always treat every candidate boundary as though it was a true boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00      1766\n",
      "        True       0.77      1.00      0.87      5769\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      7535\n",
      "   macro avg       0.38      0.50      0.43      7535\n",
      "weighted avg       0.59      0.77      0.66      7535\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_hat_baseline = [classifier.baseline_classifier(o) for o in data.load_candidates(data.file_as_string(\"data/wsj_sbd/dev.txt\"))]\n",
    "print(classification_report(y_dev, y_hat_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline #2: Next-token capitalization\n",
    "\n",
    "What if we say that a candidate is a boundary if the following token is capitalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.60      0.40      0.48      1766\n",
      "        True       0.83      0.92      0.87      5769\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      7535\n",
      "   macro avg       0.71      0.66      0.67      7535\n",
      "weighted avg       0.78      0.80      0.78      7535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat_next_tok_cap = [classifier.next_tok_capitalized_baseline(o) for o in data.load_candidates(data.file_as_string(\"data/wsj_sbd/dev.txt\"))]\n",
    "print(classification_report(y_dev, y_hat_next_tok_cap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline #3: Punkt\n",
    "\n",
    "Using the [NLTK implementation](http://www.nltk.org/_modules/nltk/tokenize/punkt.html) of [Punkt (Kiss & Strunk, 2006)](https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2006.32.4.485):\n",
    "\n",
    "(Note: You'll need to have downloaded the NLTK pre-trained Punkt model for this baseline to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.43      0.59      1766\n",
      "        True       0.85      0.99      0.91      5769\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      7535\n",
      "   macro avg       0.88      0.71      0.75      7535\n",
      "weighted avg       0.87      0.86      0.84      7535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat_punkt = [classifier.punkt_baseline(o) for o in data.load_candidates(data.file_as_string(\"data/wsj_sbd/dev.txt\"))]\n",
    "print(classification_report(y_dev, y_hat_punkt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "None of these are especially inspiring in their performance... can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Your turn!\n",
    "\n",
    "### Part 1: Now it's your turn.\n",
    "\n",
    "Your mission, in this assignment, is to implement at least two additional approaches. The approaches must be different in terms of the features you choose to use, classification approach, or both, but beyond that limitation you are free to do whatever you like as long as it is your own work (i.e., do not download and use an already-existing sentence boundary detector tool). You may, however, replicate an existing algorithm, though please be very careful not about re-using other people's code. \n",
    "\n",
    "For a discussion of features that may prove useful, and of other approaches to this task that you may wish to replicate or build off of, consult the bibliography discussed in [Gorman (2014)](http://www.wellformedness.com/blog/simpler-sentence-boundary-detection/); note that the specific sentence boundary detector described on that page uses a small set of highly useful features, but you will want to go beyond simply replicating his system.\n",
    "\n",
    "***Deliverable***: The code for your ≥ two additional classifiers, along with a brief writeup (≈1 page) of what you built and why you chose those specific approaches and features. \n",
    "\n",
    "### Part 2: Evaluation & Error analysis\n",
    "\n",
    "After building and describing your additional classifiers, evaluate their performance against the `test` partition of the data as shown above. Write a brief summary of their relative performance, and include an _error analysis_. Examine cases where your classifiers failed to predict correctly, or disagreed with one another, and see if you can identify any patterns that might explain where they were going awry.\n",
    "\n",
    "***Deliverable***: Your written summary, as well as any relevant data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier 1: _Regex All the Things!!!_ (rule-based regex classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(data.load_candidates(data.file_as_string(\"data/wsj_sbd/train.txt\")))\n",
    "dev = list(data.load_candidates(data.file_as_string(\"data/wsj_sbd/dev.txt\")))\n",
    "test = list(data.load_candidates(data.file_as_string(\"data/wsj_sbd/test.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my first classifier, I was adhering to the idea that simpler is better. Machine Learning methodologies can be incredibly powerful, but sometimes are inferior when simpler solution exists. This is because non-Machine Learning approaches can be more tractable, interpretable, and, sometimes, easier to implement/maintain. Also, sometimes it's more fun to come up with features yourself. Since sentence boundary detection seems like a simple task at face value (for humans, at least), I wanted to try a simple solution. \n",
    "\n",
    "I decided that an easy approach to this problem to use regexes to evaluate the contexts of end-sentence punctuation. For example, if the next word is capitalized or not is a decent indicator for sentence beginnings. First I created a common abbreviations list, `common_abbr`. This alone, actually, yielded surprising results, with results in the mid-90s. I had expected that my regex approach could get good results with respect to its simplicity, but since I had gotten such good results in my first version I decided to continue honing this rule-based method to see how far I could push it. (Note: in the first iteration `common_abbr` included titles, like \"Ms\" and \"Prof\", though they were moved to a dedicated list in a future iteration.)  \n",
    "\n",
    "To improve the model I adjusted the rules (ways the regex matching was being interpreted) and I added as many more abbreviations/titles as I could think of. I looked at the examples that were classified incorrectly, searching not for specific instances of failure, but for errors that I imagined were very general. For example, my list of abbreviations did not include month abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr','Jun','Jul','Aug','Sept','Sep','Oct','Nov','Dec']\n",
    "months += [s.lower() for s in months]\n",
    "months += [s.upper() for s in months]\n",
    "months += [month+\"(\\.)*\\s*[0-9]*\" for month in months]\n",
    "misc = ['St','Pl','Ave','Rd','Ct','Blvd','Inc','Co','Ltd','No','etc','i','e','g','et','al','Dept','Corp']\n",
    "misc += [s.lower() for s in misc]\n",
    "\n",
    "common_abbr = months + misc + [\"abbr\"] # lol, almost forgot to add \"abbr\" to common_abbr :/\n",
    "common_abbr = [\".*\"+s for s in common_abbr]\n",
    "\n",
    "titles = ['Dr','Mr','Mrs','Ms','Messrs','Prof','Dir','Exec','Asst','Sen','Gov',\n",
    "          'Rep','Reps','Repr','Dep','St','Rev','Capt','Sgt','Lt']\n",
    "\n",
    "common_abbr = re.compile('|'.join(common_abbr))\n",
    "titles = re.compile('|'.join(titles))\n",
    "\n",
    "initials = re.compile('([a-zA-Z]\\.)*[a-zA-Z]')\n",
    "lowercase = re.compile(\"([a-z]|[0-9]).*\")\n",
    "    \n",
    "    \n",
    "def regex_classifier(candidate, test_output=False):\n",
    "    if common_abbr.match(candidate.left_raw) and lowercase.match(candidate.right_token) \\\n",
    "            or titles.fullmatch(candidate.left_token) \\\n",
    "            or initials.fullmatch(candidate.left_token): # not a true break\n",
    "        if test_output and candidate.is_true_break: # I was wrong\n",
    "            print(\"\\nFalse Negative\")\n",
    "            print(candidate.left_token,', ', candidate.orig_obs)\n",
    "        return False\n",
    "    \n",
    "    else: # is a true break\n",
    "        if test_output and not candidate.is_true_break:\n",
    "            print(\"\\nFalse Positive\")\n",
    "            print(candidate.left_token,', ', candidate.orig_obs)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After honing this rule-based approach I was able to get my overall accuracy up to 99%, which I was happy with, especially considering this regex approach was originally meant as a precursor to a more complex Machine Learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97      9453\n",
      "        True       0.99      0.99      0.99     34826\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     44279\n",
      "   macro avg       0.98      0.98      0.98     44279\n",
      "weighted avg       0.99      0.99      0.99     44279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98      1766\n",
      "        True       1.00      0.99      0.99      5769\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      7535\n",
      "   macro avg       0.98      0.99      0.99      7535\n",
      "weighted avg       0.99      0.99      0.99      7535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98      2205\n",
      "        True       0.99      0.99      0.99      7531\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      9736\n",
      "   macro avg       0.99      0.99      0.99      9736\n",
      "weighted avg       0.99      0.99      0.99      9736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = [o.is_true_break for o in train]\n",
    "results_train = [regex_classifier(o) for o in train]\n",
    "print(classification_report(y_train, results_train))\n",
    "\n",
    "y_dev = [o.is_true_break for o in dev]\n",
    "results_dev = [regex_classifier(o) for o in dev]\n",
    "print(classification_report(y_dev, results_dev))\n",
    "\n",
    "y_test = [o.is_true_break for o in test]\n",
    "results_test = [regex_classifier(o) for o in test]\n",
    "print(classification_report(y_test, results_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier 2: _Feed the Regex to a Support Vector Machine!!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the regex approach worked even better than I thought it would. So well, in fact, that I ended up putting in more work/spending more time on that than I originally intended. I also improved it more that I original intended/thought I could. So, I decided that an interesting second approach would be to take these regex features that served me so well in my first attempt and give them to a Support Vector Machine, to see if an SVM could learn better rules (coefficients) than I was able to manufacture manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4\n",
    "\n",
    "def gen_feature_vector(candidate, fvec):\n",
    "    fvec[0] = +1 if common_abbr.match(candidate.left_raw)    else -1\n",
    "    fvec[1] = +1 if lowercase.match(candidate.right_token)   else -1\n",
    "    fvec[2] = +1 if titles.fullmatch(candidate.left_token)   else -1\n",
    "    fvec[3] = +1 if initials.fullmatch(candidate.left_token) else -1\n",
    "#     fvec[4] = +1 if lowercase.match(candidate.left_token)    else -1\n",
    "    \n",
    "def gen_all_feature_vectors(data):\n",
    "    feature_vectors = np.empty((len(data),n_features))\n",
    "\n",
    "    for item,vector in zip(data,feature_vectors):\n",
    "        gen_feature_vector(item, vector)\n",
    "    \n",
    "    return feature_vectors, np.asarray([o.is_true_break for o in data])\n",
    "\n",
    "def classifier_test(classifier, x, y):\n",
    "    y_hat = classifier.predict(x)\n",
    "    print(classification_report(y_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = gen_all_feature_vectors(train)\n",
    "x_dev,   y_dev   = gen_all_feature_vectors(dev)\n",
    "x_test,  y_test  = gen_all_feature_vectors(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = svm.SVC(kernel=\"linear\")\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97      9473\n",
      "        True       0.99      0.99      0.99     34806\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     44279\n",
      "   macro avg       0.98      0.98      0.98     44279\n",
      "weighted avg       0.99      0.99      0.99     44279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.97      0.98      1800\n",
      "        True       0.99      1.00      0.99      5735\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      7535\n",
      "   macro avg       0.99      0.98      0.99      7535\n",
      "weighted avg       0.99      0.99      0.99      7535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98      2194\n",
      "        True       0.99      0.99      0.99      7542\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      9736\n",
      "   macro avg       0.99      0.99      0.99      9736\n",
      "weighted avg       0.99      0.99      0.99      9736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_test(classifier, x_train, y_train)\n",
    "classifier_test(classifier, x_dev, y_dev)\n",
    "classifier_test(classifier, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are almost identical the results from my regex model. In fact, they are exactly identical, except that the \"precision\" and \"recall\" columns are switched, but only on the dev set (those columns are identical for the train and test sets so \"switching\" doesn't really mean anything there). Even though the columns are switched, though, they still contain the exact same values that they did for the regex classifier. It is possible that the SVM is prioritizing recall over precision somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction\n",
    "\n",
    "When I first created the features for the SVM, I added a feature for the `lowercase` regex matching the right token, as well as the left, even though this was not considered in my regex classifier. It did not have any impact on the SVM though, so I commented it out.  \n",
    "I decided, then, to go through all the other features, removing them individually, to see how they affected the SVM's ability to learn. All the remaining features had significant impacts, greatly reducing the SVM accuracies when removed, except for the `common_abbr` regex. This surprised me, since it was such an important part of my first classifier, so I decided to include these results.  \n",
    "I also experimented with different kernels for the SVMs, with full and reduced features sets, but did not include those tests since the results were the same, regardless of kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove first feature (common_abbr) from all feature vectors\n",
    "x_train = x_train[:,1:]\n",
    "x_dev = x_dev[:,1:]\n",
    "x_test = x_test[:,1:]\n",
    "\n",
    "n_features -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = svm.SVC(kernel=\"linear\")\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Reduced Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97      9473\n",
      "        True       0.99      0.99      0.99     34806\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     44279\n",
      "   macro avg       0.98      0.98      0.98     44279\n",
      "weighted avg       0.99      0.99      0.99     44279\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.97      0.98      1800\n",
      "        True       0.99      1.00      0.99      5735\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      7535\n",
      "   macro avg       0.99      0.98      0.99      7535\n",
      "weighted avg       0.99      0.99      0.99      7535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98      2194\n",
      "        True       0.99      0.99      0.99      7542\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      9736\n",
      "   macro avg       0.99      0.99      0.99      9736\n",
      "weighted avg       0.99      0.99      0.99      9736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_test(classifier, x_train, y_train)\n",
    "classifier_test(classifier, x_dev, y_dev)\n",
    "classifier_test(classifier, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
