{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Machine Translation\n",
    "Spencer Hann  \n",
    "CS 562 | Winter 2019\n",
    "\n",
    "## Part 1: IBM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 21s, sys: 792 ms, total: 7min 22s\n",
      "Wall time: 7min 22s\n"
     ]
    }
   ],
   "source": [
    "from model1 import Model1\n",
    "\n",
    "m = Model1()\n",
    "%time m.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f words translated:\n",
      "AFFAIRES \t AFFAIRS\n",
      "AMENDEMENT \t AMENDMENT\n",
      "AUTOCHTONES \t ABORIGINAL\n",
      "CHAMBRE \t HOUSE\n",
      "CHEF \t\t LEADER\n",
      "COMITÉ \t\t COMMITTEE\n",
      "COMMERCE \t TRADE\n",
      "COMMUNES \t COMMONS\n",
      "DROITS \t\t RIGHTS\n",
      "DÉCLARATION \t STATEMENT\n",
      "DÉFENSE \t DEFENCE\n",
      "DÉVELOPPEMENT \t DEVELOPMENT\n",
      "ENFANTS \t CHILDREN\n",
      "FINANCES \t FINANCE\n",
      "FÉDÉRAL \t FEDERAL\n",
      "GENS \t\t PEOPLE\n",
      "GOUVERNEMENT \t GOVERNMENT\n",
      "GUERRE \t\t WAR\n",
      "HISTOIRE \t HISTORY\n",
      "INTERNATIONALE \t INTERNATIONAL\n",
      "JUGE \t\t JUSTICE\n",
      "MINISTÈRE \t DEPARTMENT\n",
      "MONDE \t\t WORLD\n",
      "NATIONALE \t NATIONAL\n",
      "PARLEMENT \t PARLIAMENT\n",
      "PAROLE \t\t SPEAK\n",
      "PARTIE \t\t PART\n",
      "PREMIER \t PRIME\n",
      "PREMIÈRE \t FIRST\n",
      "PROGRAMME \t PROGRAM\n",
      "PROJET \t\t BILL\n",
      "PROVINCE \t PROVINCE\n",
      "PRÉSIDENT \t SPEAKER\n",
      "QUÉBEC \t\t QUEBEC\n",
      "RAISON \t\t REASON\n",
      "RAPPORT \t REPORT\n",
      "RESPONSABILITÉ \t RESPONSIBILITY\n",
      "RÉGIME \t\t PLAN\n",
      "RÉGION \t\t REGION\n",
      "RÉPONSE \t ANSWER\n",
      "SECTEUR \t SECTOR\n",
      "SERVICES \t SERVICES\n",
      "SOCIÉTÉ \t SOCIETY\n",
      "SÉCURITÉ \t SECURITY\n",
      "SÉNAT \t\t SENATE\n",
      "SÉNATEUR \t SENATOR\n",
      "TRAVAIL \t WORK\n",
      "ÉGALEMENT \t ALSO\n",
      "ÉTATS-UNIS \t STATES\n",
      "ÉTUDE \t\t STUDY\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nf words translated:\")\n",
    "with open(\"./data/fwords.txt\") as f:\n",
    "    for word in f:\n",
    "        word = word.strip()\n",
    "        tabs = '\\t' if len(word) > 6 else '\\t\\t' # column spacing\n",
    "        print(word, tabs, m.translate_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, I made a heavy use of the `collections.defaultdict` object. At first I relied too heavily on the convience of this data structure, and my memory would overflow long before making it through the entire data set. For my maximization step I was looping over every possible pair of words between the source and target vocabularies, not only did this require a lot of memory, most of that memory was taken up by dictionary entries that simply held `0`. Initially I tried putting a lower threshold on whether or not some count would \"count\". For example, I would only store some probability $p(w_t | w_s)$ if $count(w_t | w_s) > threshold$. I experiment with different thresholds, but even when it was quite high, I still would run out of RAM. I realized that the line of code `if count[e][f] > 0:` would still cause the `defaultdict` to create a new entry for `[e][f]`. When I changed that line to `if f in count[e]:`, my program began to run very smoothly. I included some other optimizations as well, like, rather than using a generator in one of the inner loops, I create a list using the generator at the top of the function so that the list can be re-used with out the sentence pairs having to be re-generated every iteration.  \n",
    "\n",
    "After the first iteration, pretty much everything translates to \"`THE`\". My model begins to produce relatively good translations in as few as 2 iterations, but there are still some hold-outs. For example, \"`PRÉSIDENT`\" still maps to \"`THE`\" until iteration 4, at which point it maps to \"`SPEAKER`\". It seems to learn most of what it can learn in the first several iterations, but may require more to learn words beyond these 50.  \n",
    "\n",
    "I trained it up to 25 iterations, nothing much changes after 4 iterations (\"`ÉTATS-UNIS`\" now maps to \"`UNITED`\" instead of \"`STATES`\"), and \"`PRÉSIDENT`\" still maps to \"`SPEAKER`\", making me think that, at least for this corpus, in these contexts \"`SPEAKER`\" is not a bad translation. This isn't totally surprising since these documents are governmental procedings from Canada, which has a Prime Minister instead of a President, unlike France and the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 57s, sys: 1.75 s, total: 31min 59s\n",
      "Wall time: 31min 59s\n"
     ]
    }
   ],
   "source": [
    "%time m.train(20) # additional 20 iterations on existing model pre-trained for 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f words translated:\n",
      "AFFAIRES \t AFFAIRS\n",
      "AMENDEMENT \t AMENDMENT\n",
      "AUTOCHTONES \t ABORIGINAL\n",
      "CHAMBRE \t HOUSE\n",
      "CHEF \t\t LEADER\n",
      "COMITÉ \t\t COMMITTEE\n",
      "COMMERCE \t TRADE\n",
      "COMMUNES \t COMMONS\n",
      "DROITS \t\t RIGHTS\n",
      "DÉCLARATION \t STATEMENT\n",
      "DÉFENSE \t DEFENCE\n",
      "DÉVELOPPEMENT \t DEVELOPMENT\n",
      "ENFANTS \t CHILDREN\n",
      "FINANCES \t FINANCE\n",
      "FÉDÉRAL \t FEDERAL\n",
      "GENS \t\t PEOPLE\n",
      "GOUVERNEMENT \t GOVERNMENT\n",
      "GUERRE \t\t WAR\n",
      "HISTOIRE \t HISTORY\n",
      "INTERNATIONALE \t INTERNATIONAL\n",
      "JUGE \t\t JUSTICE\n",
      "MINISTÈRE \t DEPARTMENT\n",
      "MONDE \t\t WORLD\n",
      "NATIONALE \t NATIONAL\n",
      "PARLEMENT \t PARLIAMENT\n",
      "PAROLE \t\t SPEAK\n",
      "PARTIE \t\t PART\n",
      "PREMIER \t PRIME\n",
      "PREMIÈRE \t FIRST\n",
      "PROGRAMME \t PROGRAM\n",
      "PROJET \t\t BILL\n",
      "PROVINCE \t PROVINCE\n",
      "PRÉSIDENT \t SPEAKER\n",
      "QUÉBEC \t\t QUEBEC\n",
      "RAISON \t\t REASON\n",
      "RAPPORT \t REPORT\n",
      "RESPONSABILITÉ \t RESPONSIBILITY\n",
      "RÉGIME \t\t PLAN\n",
      "RÉGION \t\t REGION\n",
      "RÉPONSE \t ANSWER\n",
      "SECTEUR \t SECTOR\n",
      "SERVICES \t SERVICES\n",
      "SOCIÉTÉ \t SOCIETY\n",
      "SÉCURITÉ \t SECURITY\n",
      "SÉNAT \t\t SENATE\n",
      "SÉNATEUR \t SENATOR\n",
      "TRAVAIL \t WORK\n",
      "ÉGALEMENT \t ALSO\n",
      "ÉTATS-UNIS \t UNITED\n",
      "ÉTUDE \t\t STUDY\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nf words translated:\")\n",
    "with open(\"./data/fwords.txt\") as f:\n",
    "    for word in f:\n",
    "        word = word.strip()\n",
    "        tabs = '\\t' if len(word) > 6 else '\\t\\t' # column spacing\n",
    "        print(word, tabs, m.translate_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Google Translate   SYSTRAN\n",
      "0 0.8429437482062828 0.6919161984038511\n",
      "1 0.8449147212133874 0.7803216053875227\n",
      "2 0.7580761160995355 0.651501860939786\n",
      "3 0.7143679962281008 0.8241106303521183\n",
      "4 0.690348542961804 0.5664218053975602\n",
      "5 1.0492484809296132 0.9557393754442446\n",
      "6 0.7983868786305349 0.8139957963337889\n",
      "7 0.8578928092681435 0.7437868342924722\n",
      "8 0.8562839466057395 0.69712983741573\n",
      "9 0.9291564424807219 0.8135647684906822\n"
     ]
    }
   ],
   "source": [
    "from bleu import BLEU, bitext\n",
    "\n",
    "bleu_google = list()\n",
    "bleu_systran = list()\n",
    "\n",
    "with open(\"data/gtranslate.tok\") as gtran, open(\"data/e.tok\") as ref:\n",
    "    for pair in bitext(gtran, ref):\n",
    "        bleu_google.append(BLEU(*pair))\n",
    "\n",
    "with open(\"data/systran.tok\") as systran, open(\"data/e.tok\") as ref:\n",
    "    for pair in bitext(systran, ref):\n",
    "        bleu_systran.append(BLEU(*pair))\n",
    "\n",
    "print(\"  Google Translate\", \"  SYSTRAN\")\n",
    "for i,scores in enumerate(zip(bleu_google,bleu_systran)):\n",
    "    print(i,*scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply looking at the scores, it would appear that Google Translate is a significantly better translator. However, when looking at the actual sentences, their perfomances seem to be very similar. The SYSTRAN output seem to be more literal, where the Google Translate output, while not translating words directly from the french source, matches the english reference more closely. For example, where the reference translation reads \"...AND IT IS CERTAINLY A BURDEN ON...\", Google's translation reads \"...AND IT IS A BURDEN FOR...\", and SYSTRAN's translation reads, \"...AND THAT CONSTITUTES A BURDEN FOR...\". As a native English speaker, who does not understand much French, I find the \"it is a\" phrasing better/simpler/more intuitive than the \"constitutes a\", but when referencing the original French sentence which reads, \"...cela constitue un...\", I have to wonder which really is better. How hard should the translator try to interpret meaning over translate literally/faithfully? Obviously this is objective, but it highlights how BLEU is at the mercy of the reference translations used, even if the similarity metric is reliable.  \n",
    "\n",
    "The main bug I ran into with this function was actually a little silly, but no less of a headache. I originally passed `\"data/gtranslate.tok\", \"data/e.tok\"` to the `bitext` function. My output was very strange and also very uniform. I was actually just taking the BLEU score, character-wise, of the strings with `\"data/gtranslate.tok\"` as the hypothesis and `\"data/e.tok\"` as the reference. I didn't think about the fact that `bitext` was just taking in strings, not reading anything from file. This is a reason why it can be important to actually read, or at least be sure you understand, the code you use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
